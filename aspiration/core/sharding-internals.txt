.. index:: internals; sharding
.. _sharding-internals:

==================
Sharding Internals
==================

.. default-domain:: mongodb


This document introduces lower level sharding concepts for users who
are familiar with :term:`sharding` generally and want to learn more
about the internals of sharding in MongoDB. The
":doc:`/core/sharding`" document provides an overview of higher level
sharding concepts while the ":doc:`/administration/sharding`" provides
an overview of common administrative tasks.

.. index:: shard key; internals
.. _sharding-internals-shard-keys:

Shard Keys
----------

Shard keys are the field in the collection that MongoDB uses to
distribute :term:`documents` among a shard cluster. See the
:ref:`overview of shard keys <sharding-shard-keys>` for an
introduction these topics.

.. index:: shard key; cardinality
.. _sharding-shard-key-cardinality:

Cardinality
~~~~~~~~~~~

Cardinality refers to the property of the data set that allows MongoDB
to split it into :term:`chunks`. For example, consider a collection
of data such as an "address book" that stores address records:

- Consider using a ``state`` field: 

  This would hold the US state for an address document, as a shard
  key. This field has a *low cardinality*. All documents that have the
  same value in the ``state`` field *must* reside on the same shard,
  even if the chunk exceeds the chunk size.

  Because there are a limited number of possible values for this
  field, it is easier for your data may not be evenly distributed, you
  risk having data distributed unevenly among a fixed or small number
  of chunks. In this may have a number of effects:

  - If MongoDB cannot split a chunk because it all of its documents
    have the same shard key, migrations involving these chunk will take
    longer than other migrations, and it will be more difficult for
    your data to balance evenly.
    
  - If you have a fixed maximum number of chunks you will never be
    able to use more than that number of shards for this collection.

- Consider using the ``postal-code`` field (i.e. zip code:) 

  While this field has a large number of possible values, and thus has
  *higher cardinality,* it's possible that a large number of users
  could have the same value for the shard key, which would make this
  chunk of users un-splitable.

  In these cases, cardinality depends on the data. If your address book
  stores records for a geographically distributed contact list
  (e.g. "Dry cleaning businesses in America,") then a value like
  ``postal-code`` would be sufficient. However, if your address book is
  more geographically concentrated (e.g "ice cream stores in Boston
  Massachusetts,") then you may have a much lower cardinality.

- Consider using the ``phone-number`` field:
  
  The contact's telephone number has a *higher cardinality,* because
  most users will have a unique value for this field, MongoDB will be
  able to split in as many chunks as needed.

While "high cardinality," is necessary for ensuring an even
distribution of data, having a high cardinality does not garen tee
sufficient :ref:`query isolation <sharding-shard-key-query-isolation>`
or appropriate :ref:`write scaling
<sharding-shard-key-write-scaling>`. Continue reading for more
information on these topics.

.. index:: shard key; write scaling
.. _sharding-shard-key-write-scaling:

Write Scaling
~~~~~~~~~~~~~

TODO add ObjectID to glossary.

Some possible shard keys will allow your application to take advantage of
the increased write capacity that the shard cluster can provide, while
others do not. Consider the following example where you shard by the
default :term:`_id` field, which holds an :term:`ObjectID`.

The ``ObjectID`` holds a value, computed upon creation, that is a
unique identifier for the object. However, the most significant data in
this value a is time stamp, which means that they increment
in a regular and predictable pattern. Even though this value has
:ref:`high cardinality <sharding-shard-key-cardinality>`, when
this, or *any date or other incrementing number* as the shard key all
insert operations will always end up on the same shard. As a result,
the capacity of this node will define the effective capacity of the
cluster.

In most cases want to avoid these kinds of shard keys, except in some
situations: For example if you have a very low insert rate, most of
your write operations are :func:`update() <db.collection.update()>`
operations distributed throughout your entire data set, **and** you're
sharding to support a large data set. Instead, choose shard keys that
have *both* high cardinality and that will generally distribute write
operations across the *entire cluster*.

Typically, a computed shard key that has some amount of "randomness,"
such as ones that include a cryptograpphic hash (i.e. MD5 or SHA1) of
other content in the document, will allow the cluster to scale write
operations. However, random shard keys do not typically provide
:ref:`query isolation <sharding-shard-key-query-isolation>`, which is
another important characteristic of shard keys.

Querying
~~~~~~~~

The :program:`mongos` provides an interface for applications that use
sharded database instances. The :program:`mongos` hides all of the
complexity of :term:`partitioning <partition>` from the
application. The :program:`mongos` receives queries from applications,
and then using the metadata from the :ref:`config server
<sharding-config-database>` to route the query to the
:program:`mongod` instances that provide the :term:`shards
<shard>`. While the :program:`mongos` succeeds in making all querying
operational in sharded environments, the :term:`shard key` you select
can have a profound affect on query performance.

.. seealso:: The ":ref:`mongos and Sharding <sharding-mongos>`" and
   ":ref:`config server <sharding-config-server>`" sections for a more
   general overview of querying in sharded environments.

.. index:: shard key; query isolation
.. _sharding-shard-key-query-isolation:

Query Isolation
```````````````

The fastest queries in a sharded environment are those that
:program:`mongos` will route to a single shard, using the :term:`shard
key` and the cluster meta data from the :ref:`config server
<sharding-config-server>`. Otherwise, :program:`mongos` must query all
shards, wait for them to respond and then return the result to the
application, which can be a long running operation.

If your query includes the first component of a compound :term:`shard
key` [#shard-key-index], then the :program:`mongos` can route the
query directly to a single shard, or a small number of shards, which
provides much greater performance. Even you query values of the shard
key that reside in different chunks, the :program:`mongos` will route
queires directly to the specific shard.

To select a shard key for a collection: determine which fields your
queries select by most frequently and then which of these operations
are most performance dependent. If this field is not sufficiently
selective (i.e. has low cardinality) you can add a second field to the
compound shard key to make the cluster more splitable.

.. see:: ":ref:`sharding-mongos`" for more information on query
   operations in the context of sharded clusters.

.. [#shard-key-index] In many ways, you can think of the shard key a
   cluster-wide unique index. However, be aware that sharded systems
   cannot enforce cluster-wide unique indexes *unless* the unique
   field is in the shard key. Consider the ":wiki:`Indexes`" wiki page
   for more information on indexes and compound indexes.

Sorting
```````

If you use the :func:`sort() <cursor.sort()>` method on a query in a
sharded MongoDB environment *and* the sort is on a field that is *not*
part of the shard key, then the :program:`mongos` must send the query
to all :program:`mongod` instances in the cluster, wait for a response
from every shard before it can merge the results and return data. If
you require high performance sorted queries, ensure that the sort key
is a component of the shard key.  

TODO fact check

Operations and Reliability
~~~~~~~~~~~~~~~~~~~~~~~~~~

The most important consideration when choosing a :term:`shard key`
are:

- to ensure that MongoDB will be able to distribute data evenly among
  the shards,

- to scale writes across the cluster, and 

- to ensure that :program:`mongos` can isolate most to specific
  :program:`mongod` instances.

In addition, consider the following operation consideration that the
shard key can affect.

Because each shard should be a :term:`replica set`, if a specific
:program:`mongod` instance fails, the replica set will elect another
member of that set to :term:`primary` and continue function. However,
if an entire shard is unreachable or fails for some reason then that
data will be unavailable. If your shard key distributes data required
for every operation throughout the cluster, then the failure of the
entire shard will render the entire cluster unusable. By contrast, if
the shard key allows the :program:`mongos` to isolate most operations
to a single shard, then the failure of a single will lead to only
partial unavailability.

In essence, this concern for reliably simply underscores the
importance of choosing a shard key that isolates query operations to a
single shard.

.. _sharding-internals-choose-shard-key:

Choosing a Shard Key
~~~~~~~~~~~~~~~~~~~~

It is unlikely that any single, naturally occurring key in your
collection will satisfy all requirements of a good shard key. There
are three options:

#. Compute a more ideal shard key in your application layer,
   and store this in all of your documents, potentially in the
   ``_id`` field.

#. Use a compound shard key, that uses two or three values from all
   documents that provide the right mix of cardinality with scalable
   write operations and query isolation.

#. Determine that the concerns of using a less than ideal shard key,
   is insignificant in your use case given limited write volume,
   expected data size, or query patterns and demands.

From a decision making stand point, begin by finding the the field
that will provide the required :ref:`query isolation
<sharding-shard-key-query-isolation>`, ensure that :ref:`writes will
scale across the cluster <sharding-shard-key-query-isolation>`, and
then add an additional field to provide additional :ref:`cardinality
<sharding-shard-key-cardinality>` if your primary key does not have
split-ability.

.. index:: balancing; internals
.. _sharding-balancing-internals:

Sharding Balancer
-----------------

The :ref:`balancer <sharding-balancing>` process is responsible for
redistributing chunks evenly among the shards and ensuring that each
member of the cluster is responsible for the same amount of data. 

This section contains complete documentation of the balancer process
and operations. For a higher level introduction see
the :ref:`Balancing <sharding-balancer>` section. 

Balancing Internals
~~~~~~~~~~~~~~~~~~~

The balancer originates from an arbitrary :program:`mongos`
instance. Because your shard cluster can have a number of
:program:`mongos` instances, when a balancer process is active it
creates a "lock" document in the ``locks`` collection of the
``config`` database on the :term:`config server`.

By default, the balancer process is always running. When the number of
chunks in a collection is unevenly distributed among the shards, the
balancer begins migrating :term:`chunks` from shards with a
disproportionate number of chunks to a shard with fewer number of
chunks. The balancer will continue migrating chunks, one at a time
beginning with the shard that has the lowest shard key, until the data
is evenly distributed among the shards (i.e. the difference between
any two chunks is less than 2 chunks.)

While these automatic chunk migrations crucial for distributing data
they carry some overhead in terms of bandwidth and system workload,
both of which can impact database performance. As a result, MongoDB
attempts to minimize the effect of balancing by only migrating chunks
when the disparity between numbers of chunks on a shard is greater
than 8.

.. index:: balancing; migration

The migration process ensures consistency and maximize availability of
chunks during balancing: when MongoDB begins migrating a chunk, the
database begins copying the data to the new server and tracks incoming
write operations. After migrating the chunks, the "from"
:program:`mongod` sends all new writes, to the "to" server, and *then*
updates the chunk record in the :term:`config database` to reflect the
new location of the chunk.

.. index:: sharding; chunk size
.. _sharding-chunk-size:

Chunk Size
~~~~~~~~~~

TODO link this section to <glossary:chunk size>

The default :term:`chunk` size in MongoDB is 64 megabytes.

When chunks grow beyond the :ref:`specified chunk size
<sharding-chunk-size>` a :program:`mongos` instance will split the
chunk in half, which will eventually lead to migrations, when chunks
become unevenly distributed among the cluster, the :program:`mongos`
instances will initiate a round migrations to redistribute data in the
cluster.

Chunk size is somewhat arbitrary and must account for the
following effects:

#. Small chunks lead to a more even distribution of data at the
   expense of a larger number of more frequent migrations, which
   creates expense at the query routing (:program:`mongos`) layer.  or

#. Large chunks lead to fewer migrations, which is more efficient both
   from the networking perspective *and* in terms internal overhead at
   the query routing layer. Large chunks produce these efficiencies at
   the expense of a potentially more uneven distribution of data.

For many deployments it makes sense to avoid frequent and potentially
spurious migrations at the expense of a slightly less evenly
distributed data set, but this value is :ref:`configurable
<sharding-modify-chunk-size>`. Be aware of the following limitations
when modifying chunk size:

- Automatic splitting only occurs when inserting :term:`documents
  <document>` or updating existing documents; if you lower the chunk
  size it may take for all chunks to split to the new size.

- Splits cannot be "undone:" if you increase the chunk size, existing
  chunks must grow through insertion or updates until they reach the
  new size.

Shard Size
~~~~~~~~~~

By default MongoDB will attempt to fill all available disk space with
data on every shard as the data set grows. Monitor disk utilization in
addition to other performance metrics, to ensure that the cluster
always has capacity to accommodate additional data.

You can also configure a "maximum size" for any shard when you add the
shard using the ``maxSize`` parameter of the :dbcommand:`addShard`
command.

TODO how does a system balance shards if one shard can only hold 60% of the chunks of the others?

TODO how does the system handle

.. seealso:: ":doc:`/administration/monitoring`."
