.. index:: sharding

=====================
Sharding Fundamentals
=====================

.. default-domain:: mongodb

MognoDB's sharding allows users to :term:`partition` the
data of a :term:`collection` within a database so that the documents
are automatically distributed among a number of :program:`mongod`
instances. These systems provides a larger write capacity and allows a
single database instance to have a larger working set or total data
size than a single instance could supply.

This document provides an overview of the fundamental concepts and
operation of sharding with MongoDB.

.. seealso:: The ":doc:`/sharding`" index for a list of all documents
   in this manual that contain information related to the operation
   and use of shard clusters in MongoDB.

   If you are not yet familiar with sharding, see the :doc:`Sharding
   FAQ </faq/sharding>`.

Overview
--------

Features
~~~~~~~~

With sharding MongoDB automatically distributes data among a
collection of :program:`mongod` instances. Sharding, as implemented in
MongoDB has the following features:

.. glossary::

   Range Based Sharding
      MongoDB uses "range based" sharding, where the database
      distributes documents among :term:`shards <shard>` based on the value
      of the :ref:`shard key <sharding-shard-key>`. Each :term:`chunk`
      represents a block of :term:`document` with values
      that fall within a specific range. When chunks grow beyond the
      :ref:`chunk size <sharding-chunk-size>`, MongoDB divides the
      chunks into smaller chunks (i.e. :term:`splitting <split>`)
      based on the shard key.

   Automatic Sharding
      The sharding system will automatically balance data across the
      cluster, without intervention from the application
      layer. Effective automatic sharding depends on a well chosen
      :ref:`shard key <sharding-shard-key>`, but requires no
      additional complexity, modifications, or intervention for
      developers.

   Transparent Sharding
      Sharding is completely transparent to the application layer,
      because all connections to a sharded cluster go through the
      :program:`mongos` instances. Sharding in MongoDB requires some
      :doc:`basic initial configuration </administration/sharding>`,
      but ongoing function is entirely transparent to the application.

   Sharding Capacity
      Sharding increases capacity in two ways:

      #. Given an even distribution of data with an effective
         :term:`shard key`, sharding can provide additional write
         capacity by increasing the number of :program:`mongod`
         instances.

      #. Give a shard key with sufficient :ref:`cardinality
         <sharding-shard-key-cardinality>`, sharding makes it possible
         to distribute data among a collection of :program:`mongod`
         instances, and increase the potential amount of data to mange
         with MongoDB and expand the :term:`working set`.

A typical :term:`shard cluster` consists of the config servers that
provide metadata that maps :term:`chunks <chunk>` to shards, the
:program:`mongod` instances that hold the data (i.e the :term:`shards
<shard>`,) and lightweight routing processes, :doc:`mongos
</reference/mongos>`, that routes operations to the correct shard
based on the operation and the cluster metadata.

Indications
~~~~~~~~~~~

While sharding is a very powerful and compelling feature, it comes with
significant :ref:`infrastructure requirements <sharding-requirements>`
and some limited complexity costs. As a result its important to use
sharding only as neccessary,and when indicated by actual operational
requirements. Consider the following overview of indications, or a
simple "*when you should shard,*" guide.

You should consider deploying a :term:`shard cluster`, if:

- your data set exceeds the storage capacity of a single node in your
  system.

- the size of your system's active :term:`working set` *will soon*
  exceed the capacity of the *maximum* amount of RAM for your system.

- your system has a large amount of write activity, and cannot write
  data fast enough to meet demand, and all other approaches have not
  reduced the contention.

TODO factcheck; make sure there aren't addition indications.

If these are not true of your system, sharding may add too much
complexity to your system without providing much benefit to your
system.

.. warning::

   Do not attempt to shard a collection when your system has already
   reached ore exceeded its capacity. Sharding takes some time to
   configure, and the :term:`balancer` process can take some time to
   migrate data and distribute content among the cluster.

   As a result if you know you're going to need sharding eventually,
   its crucial that you **do not** wait until your system is
   overcapacity to enable sharding.

.. index:: sharding; requirements
.. _sharding-requirements:

Requirements
------------

.. _sharding-requirements-infrastructure:

Infrastructure
~~~~~~~~~~~~~~

A :term:`shard cluster` has the following components:

- Three :term:`config servers <config database>`.

  These special :program:`mongod` instances store the configuration
  meta data for the cluster. The :program:`mongos` instances cache
  this data, and use it to determine which :term:`shard` is
  responsible for which :term:`chunk`.

  For testing purposes you can deploy a shard cluster with a single
  configuration server, but this is not recommended for production.

- One or more :program:`mongos` instances.

  These nodes cache cluster configuration from the configuration
  servers and direct queries from the application layer to the
  :program:`mongod` instances that hold the data.

  .. note::

     :program:`mongos` *Resource use:*

     In most situations :program:`mongos` instances use minimal
     resources, and you can run them on your application servers
     without impacting application performance. However, if you use
     the :term:`aggregation framework` some processing may occur on
     the :program:`mongos` instances which will cause them to require
     more system resources.

- Two or more :program:`mongod` instances, to hold data.

  These are "normal," :program:`mongod` instances that hold all of the
  actual data for the cluster.

  Typically a :term:`replica sets <replica set>` provides each individual shard,
  which provides redundancy for all data in the cluster and increases
  the overall reliability and robustness of the cluster.

  .. warning::

     MongoDB enables data :term:`partitioning <partition>`
     (i.e. sharding) on a *per collection* basis. You *must* access
     all data in a sharded cluster via the :program:`mongos`.

Data
~~~~

In most cases, there must be a significant quantity of data for
sharding to have an effect on your collection. The default
:term:`chunk` size is 64 megabytes, [#chunk-size]_ and the
:ref:`balancer <sharding-balancing>` will not kick in until there is a
difference of 8 chunks between the shard with the most number of
chunks and the shard with the least number of chunks.

Practically, this means that unless there is 512 megabytes of data,
all of the data will remain on the same shard. You can set a smaller
chunk size, or create splits in your collection using the
:func:`sh.splitFind()` or :func:`sh.splitAt()` operations in the
:program:`mongo` shell to facilitate chunks, but these defaults are
explicitly configured to prevent unnecessary splitting or migrations.

While there are some exceptional situations where you may need to
shard a small collection of data, most of the time the additional
complexity added by sharding is not worth the operational costs unless
you need the additional concurrency/capacity for some reason. If you
have a small data set, the chances are that a properly configured
single MongoDB instance or replica set will be more than sufficient
for your data service needs.

TODO link this section to <glossary:chunk size>

.. index:: chunk size
   single: sharding; chunk size

.. _sharding-chunk-size:

.. [#chunk-size] While the default chunk size is 64 megabytes, the
   size is :option:`user configurable <mongos --chunkSize>`. When
   deciding :term:`chunk` size, MongoDB (for defaults) and users (for
   custom values) must consider that: smaller chunks offer the
   possibility for a more even data distribution, but increase the
   likelihood of chunk migrations. Larger chunks decrease the need for
   migrations, but increase the amount of time required for a chunk
   migration.

   When chunks grow beyond the :ref:`specified chunk size
   <sharding-chunk-size>` a :program:`mongos` instance will split the
   chunk in half, which will eventually lead to migrations, when
   chunks become uneavenly distributed among the cluster, the
   :program:`mongos` instances will initiate a round migrations to
   redistribute data in the cluster.

TODO link this section to <glossary:shard key>

.. index:: shard key
   single: sharding; shard key

.. _sharding-shard-key:
.. _shard-key:

Shard Keys
----------

"Shard keys" refer to the :term:`field` in a MongoDB :term:`document`
that that MongoDB uses to distribute documents among the
:term:`shards`. Shard keys, like :term:`indexes`, can be either a
single field, or may be a compound key, consisting of multiple fields.

Remember, MonoDB's sharding is range-based: each :term:`chunk` holds
documents with "shard key" within a specific range. Thus, choosing the
correct shard key can have a great impact on the performance,
capability, and functioning of your database and cluster.

Choosing a shard key is something that depends on the schema of your
data and your usage patterns. The ideal shard key:

- is easily divisible which makes it easy for MongoDB to distribute
  content among the shards. Shard keys that have a limited number of
  possible values are un-ideal, as they can result in some shards that
  are "un-splittable."

  .. see:: ":ref:`sharding-shard-key-cardinality`"

- will distribute write operations among the cluster, to prevent any
  single shard from becoming a bottleneck. Shard keys that have a high
  correlation with insert time are poor choices for this reason;
  however, shard keys that have higher "randomness" satisfy this
  requirement better.

  .. see:: ":ref:`sharding-shard-key-write-scaling`"

- will make it possible for the :program:`mongos` to return most query
  operations directly to a single *specific* :program:`mongod`
  instance. Your shard key should be the primary field used by your
  queries, and fields with a high amount of "randomness" are poor
  choices for this reason.

  .. see:: ":ref:`sharding-shard-key-query-isolation`"

The challenge when selecting the shard key is that there is not always
an obvious shard key, and that it's unlikely that a single naturally
occuing [#computed-shard-key] field in your collection will satisfy
all requirements. Computing a special-purpose shard key, or using a
compound shard key can help you find a more ideal shard key, but the
shard key always some degree of compromize.

TODO add something about the order of the shard keys

.. [#computed-shard-key] In some cases, you may consider computing a
   a more sutable shard key for each document in your application and
   adding it to all of the documents before sharding.

TODO move the remainder of this section (or most of it) to internals doc

.. index:: shard key; cardinality
.. _sharding-shard-key-cardinality:

Cardinality
~~~~~~~~~~~

Cardinality refers to the property of the data set that allows MongoDB
to split it into :term:`chunks`. For example, consider a collection
of data such as an "address book" that stores address records:

- Consider using use the ``state`` field, which holds the US state for
  the contact, as a shard key. This field has a *low cardinality*. All
  documents that had the same value in the ``state`` field *must*
  reside on the same shard, even if the chunk exceeds the chunk
  size.

  Because there are a limited number of possible values for this
  field, among which your data may not be evenly distributed, you risk
  having data distributed unevenly among a fixed or small number of
  chunks. In this may have a number of effects:

  - If one chunk has a fixed size that grows beyond the chunk size,
    migrations involving this chunk will take longer than other
    migrations, which may affect the performance of some components of
    the cluster.

  - If you have a fixed maximum number of chunks you will never be
    able to utilize more than that number of shards for this
    collection.

- Consider using the ``postal-code``" field (i.e. zip code,) while
  this field has a large number of possible values, and thus has
  *higher cardinality,* it's possible that a large number of users
  could have the same value for the shard key, which would make this
  chunk of users un-splitable.

  In these cases, cardinality depends on the data. If your address book
  stores records for a geographically distributed contact list
  (e.g. "Dry cleaning businesses in America,") then a value like
  ``postal-code`` would be sufficient. However, if your address book is
  more geographically concentrated (e.g "ice cream stores in Boston
  Massachusetts,") then you may have a much lower cardinality.

- Consider using the "``phone-number``" field, which would hold the
  contact's telephone number. This number has a *higher cardinality,*
  because every (or most) users will have different values for this
  field, MongoDB will be able to split in as many chunks as
  needed.

While "high cardinality," is necessary for ensuring an even
distribution of data, having a high cardinality does not garen tee
sufficient :ref:`query isolation <sharding-shard-key-query-isolation>`
or appropriate :ref:`write scaling <sharding-shard-key-write-scaling>`.

.. index:: shard key; write scaling
.. _sharding-shard-key-write-scaling:

Write Scaling
~~~~~~~~~~~~~

TODO add ObjectID to glossary.

Some potential shard keys allow your application to take advantage of
the increased write capacity that the shard cluster can provide, while
others do not. Consider using the default :term:`_id` field, which
holds an :term:`ObjectID`.

The ``ObjectID`` holds a value, computed upon creation, that is a
unique identifier for the object. However, the most significant data in
this value is effectively a time stamp, which means that they increment
in a regular and predictable pattern. Even though this value has
:ref:`high cardinality <sharding-shard-key-cardinality>`, when
this, or *any date or other incrementing number* as the shard key all
insert operations will always end up on the same shard. As a result,
the capacity of this node will become the effective capacity of the
cluster.

Unless you have a very low insert rate, most of your write operations
are :func:`update()` operations distributed throughout your entire
data set, **and** you're sharding to support a large data set, avoid
these date-based shard keys. Instead, choose shard keys that have both
high cardinality and that will generally distribute write operations
among the *entire cluster*.

Typically, a shard key that has some amount of "randomness," like a
cryptographic hash (i.e. MD5 or SHA1,) will provide the ability for
write scaling. However, random hashes do not typically provide
:ref:`query isolation <sharding-shard-key-query-isolation>`, which is
an important characteristic of shard keys.

Querying
~~~~~~~~

The :program:`mongos` provides an interface for applications
connecting to sharded systems that hides all of the complexity of
:term:`partionining <partion>` from the application. The
:program:`mongos` recieves queries from applications, and then using
the metadata from the config server to route the query to the
:program:`mongod` instances that power the :term:`shards
<shard>`. While the :program:`mongos` succedes in making all querying
operational in sharded environments, the :term:`shard key` you select
can have a profound profound effect on query performance.

.. seealso:: The ":ref:`mongos and Sharding <sharding-mongos>`" and
   ":ref:`config server <sharding-config-server>`" sections for a more
   general overview of querying in sharded environments.

.. index:: shard key; query isolation
.. _sharding-shard-key-query-isolation:

Query Isolation
```````````````

The fastest queries in a sharded environment are the ones that
:program:`mongos` can route to a single shard, using the :term:`shard
key` and the cluster meta data from the config database. Otherwise,
:program:`mongos` must query all shards, wait for them to respond and
then return the result to the application, which can be a long running
operation.

If your query includes the first component of a compound :term:`shard
key` [#shard-key-index], then the :program:`mongos` can route the
query directly to a single shard, or a small number of shards, which
provides much greater performance. Even you query values of the shard
key that reside in different chunks, the :program:`mongos` will route
queires directly to the specific shard.

When choosing a shard key for a collection, first examine your queries
to determine which fields your queries selct by most frequently, and
then which of these operations are most perforamance dependent. If
this field is not sufficently selective (i.e. has low cardinality) you
can add a second field to the compound shard key to make the cluster
more splitable.

.. see:: ":ref:`sharding-mongos`" for more information on query
   operations in the context of sharded clusters.

.. [#shard-key-index] In many ways, you can think of the shard key a
   cluster-wide unique index. However, be aware that sharded systems
   cannot enforce cluster-wide unique indexes *unless* the unique
   field is in the shard key.

   .. see:: The ":wiki:`Indexes`" wiki page for more information on
      indexes and compound indexes.

Sorting
```````

If you use the :func:`sort()` method on a query in a sharded MongoDB
environment *and* the sort is on a field that is *not* part of the
shard key, then the :program:`mongos` must send the query to all
:program:`mongod` instances in the cluster, wait for a response from
every shard before it can merge the results and return data. If you
require high performance sorted queries, ensure that the key you need
to sort from is in the shard key.

Operations
~~~~~~~~~~

The most important considerations when choosing a :term:`shard key`
are to ensure that MongoDB will be able to distribute data evenly
among the shards, to scale writes across the cluster, and to ensure
that :program:`mongos` can isolate most to specific :program:`mongod`
instances. However, your choice in shard key can have a couple of
operational impacts, specifically:

Reliability
```````````

Because each shard should be a :term:`replica set`, if a specific
:program:`mongod` instance fails, the replica set will elect another
member of that set to :term:`primary` and continue functiong. However,
if an entire shard is unreachable or fails for some reason then that
data will be unavalible. If your shard key distributes data required
for every operation throughout the cluster, then the failure of the
entire shard will render the entire cluster unusable. By contrast, if
the shard key allows the :program:`mongos` to isolate most operations
to a single shard, then the failure of a single will lead to only
partial data unavalibility.

In essence, this reliabily concern simply underscores the importance
of chosing a shard key that isolates query operations to a single
shard.

Index Optimization
``````````````````

TODO I need to learn more about indexing before I write this section.

Choosing a Shard Key
~~~~~~~~~~~~~~~~~~~~

It is unlikely that any single, naturally occuring key in your
collection will satisfy all requirements of a good shard key. There
are three options:

#. Compute a more ideal shard key in your application layer,
   and store this in all of your documents, potentially in the
   "``_id``" field.

#. Use a compound shard key, that uses two or three values from all
   documents that provide the right mix of cardinality with scalable
   write operations and query isolation.

#. Determine that the concerns of using a less than ideal shard key,
   is insignifigant in your use case given limited write volume,
   expected data size, or query patterns and demands.

From a decision making stand point, begin by finding the the field
that will provide the required :ref:`query isolation
<sharding-shard-key-query-isolation>`, ensure that writes will scale
across the cluster, and then add an additional field to provide
additional cardinality if your primary key does not have
splitability.

.. _sharding-config-server:

Config Servers
--------------

The configuration servers store the shard metadata that tracks the
relationship between the range that defines a :term:`chunk` and the
:program:`mongod` instance (typically a :term:`replica set`) or
:term:`shard` where that data resides. Without a config server, the
:program:`mongos` insteances are unable to route queries and write
operations, and the cluster. This section describes their operation
and use.

Config servers *do not* run as replica sets. Instead, a :term:`shard
cluster` operates with a group of *three* config servers that use a
two-phase commit process that ensures imediete consistency and
reliability. Becasue the :program:`mongos` instances all maintain
caches of the config server data, the actual traffic on the config
servers is small. MongoDB will write data to the config server only
when:

- Creating splits in existing chunks, which happens as data in
  existing chunks excedes the maximum chunk size.

- Migrating a chunk between shards.

If a *single* configuration server becomes unavavlible, the cluster's
metadata becomes *read only*. It is still possible to read and write
data from the shards, but no chunk migrations or splits will occur
until all three servers are accessible. At the same time, config
server data is only read in the following situations:

- A new :program:`mongos` starts for the first time, or an existing
  :program:`mongos` restarts.

- After a chunk migration, the :program:`mognos` instances update
  themselves with the new cluster metadata.

TODO clarify this process.

If all three config servers are inaccessible, then you can continue to
use the cluster as long as you don't restart the :program:`mongos`
isntances until the config servers are accessible. When the
:program:`mongos` instances restart and there are no accessible config
servers they are unable to direct queries or write operations to the
cluster.

Because the configuration data is small relaitve to the amount of data
stored in a cluster, the amount activity is relatively low, and 100%
uptime is not required for a functioning shard cluster, backing up the
config servers is not difficult. However, becasue the shard cluster
will become totally inoperatble if you loose your configuration
servers, precautions to ensure that the config servers remain avalible
and intact are totally crucial.

.. index:: mongos
.. _sharding-mongos:

:program:`mongos` and Querying
------------------------------

.. seealso:: ":doc:`/reference/mongos`" and the :program:`mongos`\-only
   settings: ":setting:`test`" and :setting:`chunkSize`.

Operations
~~~~~~~~~~

The :program:`mongos` provides a single unified interface to a sharded
cluster for applications using MongoDB. Except for the selection of a
:term:`shard key` application developers and administrators need not
consider any of the internal details of sharding because of the
:program:`mongos`.

:program:`mongos` caches data from the :ref:`config server
<sharding-config-server>`, and uses this to route operations from the
applications and clients to the :program:`mongod`
instances. :program:`mongos` have no *persistent* state and consume
minimal system resources.

The most common practice is to run :program:`mongos` instances on the
same systems as your application layer, but you can maintain
:program:`mongos` instances on the shard or on other dedicated
resources.

.. note::

   .. versionchanged:: 2.1

   Some aggregation operations using the :dbcommand:`aggregate` will
   cause :program:`mongos` instances to require more CPU resources
   than in previous versions. This modified performance profile may
   dictate alternate architecture decisions if you make use the
   :term:`aggregation framework` extensively in a sharded environment.

Query Routing
~~~~~~~~~~~~~

:program:`mongos` uses information from :ref:`config servers
<sharding-config-server>` to return queires in the most efficent
manner possible. MongoDB classifies all operations in a sharded
environment as "**targeted**" or "**global**, as follows:

#. *Targeted* opeations, where the :program:`mongos` sends the
   operations to specific :program:`mongod` instance based on the
   :term:`shard key`.

   For example, given a shard key of "``{ shard_id: 1 }``", the
   :program:`mongos` would target the following operations at a
   specific :program:`mongod` or a small number of :program:`mongod`
   instances:

     .. code-block:: javascript

        db.collection.find( { shard_id: 355 } )
        db.collection.find( { shard_id: 52, host: "example.org" } )
        db.collection.insert( { shard_id: 155, host: "example.net", [...] } )
        db.collection.insert( { [...] } ) // all insert operations are targeted.
        db.collection.update( { shard_id: 112, host: "example.org"} , { $inc: { counter: 1 } } )
        db.collection.remove( { shard_id: 355 } )

#. *Global* operations, where the :program:`mongos` sends the
   operations to *all* :program:`mongod` instances, and then combines
   the responses before sending them to the the application layer.
   For example, given a shard key of "``{ shard_id: 1 }``", the
   :program:`mongos` would have to send all operations to all shards
   in the cluster, as in the following:

   .. code-block:: javascript

      db.collection.find()
      db.collection.find( { host: "example.com" } )
      db.collection.update( { shard_id: 112, host: "example.org"} , { $inc: { counter: 1 } } )
      db.collection.remove( { host: "test.example.com" } )
      db.collection.ensureIndex( [...] ) // all index operations are global.

   At the same time we can also divide gobal operations into two
   categoris: *parallel* and *sequential*.

   Parallel operations must run on all shards, return to the
   :program:`mongos` which does a final operation on the results and
   returns to the application. Sequential operations, run on all
   shards, but the :program:`mongos` will begin returning data to the
   client as soon as the *first* shard returns data.

   Given a shard key of "``{ shard_id: 1 }``", the following
   operations are *parallel*:

   .. code-block:: javascript

      db.collection.find( [...] ).sort( { shard_id: 1 } )
      db.collection.find().count()

   .. note::

      :func:`count() <cursor.count()>` operations on queries
      (i.e. :func:`find() <db.collection.find()>`) may be global or
      targeted depending on the underlying query.

   Given a shard key of "``{ shard_id: 1 }``", the following
   operations are *sequential*:

   .. code-block:: javascript

      db.collection.find()
      db.collection.find( [...] ).sort( { counter: -1 } )

TODO factcheck above, cite: wiki, doesn't make sense.

While some global operations, may be unavoidable, You can provent
global operations by ensuring that you include the shard key in your
queries other and operations.

.. _sharding-balancing:

Balancing and Distribution
--------------------------

Balancing Procedure
~~~~~~~~~~~~~~~~~~~

Shard Size
~~~~~~~~~~

Balancer Operations
~~~~~~~~~~~~~~~~~~~

- turn off the balancer during peek times.

- disable the balancer specifically.

Architecture Possibilities
--------------------------

Minimal System
~~~~~~~~~~~~~~

Production System
~~~~~~~~~~~~~~~~~

Sharded and Unsharded Data
~~~~~~~~~~~~~~~~~~~~~~~~~~

- moving primaries for databases

Administrative Operations
-------------------------

TODO add link to deploy sharded cluster

TODO write this tutorial

Removing a Shard
~~~~~~~~~~~~~~~~

Adding a Shard
~~~~~~~~~~~~~~

Manually Creating Splits
~~~~~~~~~~~~~~~~~~~~~~~~

Disabling the Balancer During Peek Activity
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Disabling the Balancer for Backup Operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
