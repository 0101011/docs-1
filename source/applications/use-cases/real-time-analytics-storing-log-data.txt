=====================================
Real Time Analytics: Storing Log Data
=====================================

.. default-domain:: mongodb

Overview
--------

This document outlines the basic patterns and principals for using
MongoDB as a persistent storage engine for log data from servers and
other machine data.

Problem
~~~~~~~

Servers generate a large number of events (i.e. logging,) that contain
useful information about their operation including errors, warnings,
and users behavior. By default, most servers, store these data in
plain text log files on their local file systems.

While plain-text logs are accessible and human-readable, they are
difficult to use, reference, and analyze without holistic systems for
aggregating and storing these data.

Solution
~~~~~~~~

The solution presented by this case study assumes that each server
generates events also consumes event data and that each server can
access the MongoDB instance. Furthermore, this design assumes that the
query rate for this logging data is substantially lower than
the insert rate and optimizes accordingly. This usage profile is
common for logging applications with a high-bandwidth event stream.

.. note::

   This case assumes that you're using an standard uncapped collection
   for this event data, unless otherwise noted. See the section on
   :ref:`capped collections <rta-storing-log-data-capped-collections>`

Schema Design
~~~~~~~~~~~~~

The schema for storing log data in MongoDB depends on the format of
the event data that you're storing. For a simple example, consider
standard request logs in the combined format from the Apache HTTP
Server.  A line from these logs may resemble the following:

.. code-block:: text

   127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326 "[http://www.example.com/start.html](http://www.example.com/start.html)" "Mozilla/4.08 [en] (Win98; I ;Nav)"

The simplest approach to storing the log data would be putting the exact
text of the log record into a document:

.. code-block:: javascript

   {
     _id: ObjectId('4f442120eb03305789000000'),
    line: '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326 "[http://www.example.com/start.html](http://www.example.com/start.html)" "Mozilla/4.08 [en] (Win98; I ;Nav)"'
   }

While solution is does capture all data in a format that MongoDB can
use, the data is not particularly useful, or it's not terribly
efficient: if you need to find events that the same page, you would
need to use a regular expression query, which would require a full
scan of the collection. The preferred approach is to extract the
relevant information from the log data into individual fields in a
MongoDB :term:`document`.

When you extract data from the log into fields, pay attention to the
data types you use to render the log data into MongoDB.

As you design this schema, be mindful that the data types you use to
encode the data can have a significant impact on the performance and
capability of the logging system. Consider the date field: In the
above example, "``[10/Oct/2000:13:55:36 -0700]``" is 28 bytes long. If
you store this with the UTC timestamp type, you can convey the same
information in only 8 bytes.

Additionally, using proper types for your data also increases query
flexibility: if you store date as a timestamp you can make date range
queries, whereas it's very difficult to compare two *strings* that
represent dates. The same issue holds for numeric fields; storing
numbers as strings requires more space and is difficult to query.

Consider the following document that captures all data from the above
log entry:

.. code-block:: javascript

   {
        _id: ObjectId('4f442120eb03305789000000'),
        host: "127.0.0.1",
        logname: null,
        user: 'frank',
        time:  ,
        request: "GET /apache_pb.gif HTTP/1.0",
        status: 200,
        response_size: 2326,
        referrer: "[http://www.example.com/start.html](http://www.example.com/start.html)",
        user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"
   }

When extracting data from logs and designing a schema, also consider
what information you can omit from your log tracking system. In most
cases there's no need to track *all* data from an event log, and you
can omit other fields. To continue the above example, here the most
crucial information may be the host, time, path, user agent, and
referrer, as in the following example document:

.. code-block:: javascript

   {
        _id: ObjectId('4f442120eb03305789000000'),
        host: "127.0.0.1",
        time:  ISODate("2000-10-10T20:55:36Z"),
        path: "/apache_pb.gif",
        referer: "[http://www.example.com/start.html](http://www.example.com/start.html)",
        user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"
   }

You may also consider omitting explicit time fields, because the
``ObjectId`` embeds creation time:

.. code-block:: javascript

    {
         _id: ObjectId('4f442120eb03305789000000'),
         host: "127.0.0.1",
         time:  ISODate("2000-10-10T20:55:36Z"),
         path: "/apache_pb.gif",
         referer: "[http://www.example.com/start.html](http://www.example.com/start.html)",
         user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"
    }

System Architecture
~~~~~~~~~~~~~~~~~~~

The primary performance concern for event logging systems are:

#. how many inserts per second can it support, which limits the its
   event throughput, and

#. how will the system manage the growth of event data, particularly
   concerning a growth in insert activity.

   In most cases the best way to increase the capacity of the system
   is to use an architecture with some sort of :term:`partitioning
   <partition>` or :term:`sharding` that distributes writes among a
   cluster of systems.

Operations
----------

The main performance-critical operation in storing
an event log is the insertion speed. However, you also need to be able to
query the event data for relevant statistics. This section will describe
each of these operations, using the Python programming language and the
``pymongo`` MongoDB driver. These operations would be similar in other
languages as well.

Inserting a Log Record
~~~~~~~~~~~~~~~~~~~~~~

In many event logging applications, you might accept some degree of risk
when it comes to dropping events. In others, you need to be absolutely
sure you don't drop any events. MongoDB supports both models. In the case
where you can tolerate a risk of loss, you can insert records
*asynchronously* using a fire-and-forget model:

.. code-block:: pycon

   >>> import bson
   >>> import pymongo
   >>> from datetime import datetime
   >>> conn = pymongo.Connection()
   >>> db = conn.event_db
   >>> event = {
   ...     _id: bson.ObjectId(),
   ...     host: "127.0.0.1",
   ...     time:  datetime(2000,10,10,20,55,36),
   ...     path: "/apache_pb.gif",
   ...     referer: "[http://www.example.com/start.html](http://www.example.com/start.html)",
   ...     user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"
   ...}
   >>> db.events.insert(event, safe=False)

This is the fastest approach: this code does not require that the
MongoDB server acknowledge that it received the insert.  This approach
caries the greatest risk because the application will not detect
network and server failures, including ``DuplicateKeyErrors`` on a
unique index.

If you want to ensure that MongoDB acknowledges inserts, you can pass
``safe=True`` to the insert operation:

.. code-block:: pycon

   >>> db.events.insert(event, safe=True)

If you have a lower tolerance for data loss, you can require that
MongoDB commits all data to the on-disk journal before your
application can continue:

.. code-block:: pycon

   >>> db.events.insert(event, j=True)

.. note::

   ``j=True`` implies ``safe=True``.

Finally, if you have *extremely low* tolerance for event data loss,
you can require that MongoDB replicate the data to multiple
:term:`secondary` :term:`replica set` members before returning:

.. code-block:: pycon

   >>> db.events.insert(event, w=2)

This will force your application to acknowledge that the data has
replicated to 2 members of the :term:`replica set`. You can combine
options as well:

.. code-block:: pycon

   >>> db.events.insert(event, j=True, w=2)

In this case, the insert operations waits for both a journal commit
*and* a replication acknowledgment. Although this is the safest
option, it is also the slowest, so you should be aware of the trade
off when performing inserts.

.. note::

   If at all possible in your application architecture, you should consider
   using bulk inserts to insert event data. All the options discussed above
   apply to bulk inserts, but you can actually pass multiple events as the
   first parameter to .insert(). By passing multiple documents into a
   single insert() call, MongoDB are able to amortize the performance penalty you
   incur by using the 'safe' options such as j=True or w=2.

.. seelaso:: :term:`Write concern` and :dbcommand:`getLastError`.

Finding All the Events for a Particular Page
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Query
`````

For a web analytics-type operation, getting the logs for a particular
web page might be a common operation for which  you would want to optimize.
In this case, the query would be as follows:

.. code-block:: pycon

   >>> q_events = db.events.find({'path': '/apache_pb.gif'})

Note that the sharding setup you use (should you decide to shard this
collection) has performance implications for this operation. For
instance, if you shard on the 'path' property, then this query will be
handled by a single shard, whereas if you shard on some other property or
combination of properties, the mongos instance will be forced to do a
scatter/gather operation which involves *all* the shards.

Index Support
`````````````

This operation would benefit significantly from an index on the 'path'
attribute:

.. code-block:: pycon

   >>> db.events.ensure_index('path')

One potential downside to this index is that it is relatively randomly
distributed, meaning that for efficient operation the entire index
should be resident in RAM. Since there is likely to be a relatively
small number of distinct paths in the index, however, this will probably
not be a problem.

Finding All the Events for a Particular Date
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Query
`````

You may also want to find all the events for a particular date. In this
case, you'd perform the following query:

.. code-block:: pycon

   >>> q_events = db.events.find('time':
   ...     { '$gte':datetime(2000,10,10),'$lt':datetime(2000,10,11)})

Index Support
`````````````

In this case, an index on 'time' would provide optimal performance:

.. code-block:: pycon

   >>> db.events.ensure_index('time')

One of the nice things about this index is that it is *right-aligned*.
Since you are always inserting events in ascending time order, the
right-most slice of the B-tree will always be resident in RAM. So long
as your queries focus mainly on recent events, the *only* part of the
index that needs to be resident in RAM is the right-most slice of the
B-tree, allowing MongoDB to keep quite a large index without using up much of
the system memory.

Finding All the Events for a Particular Host/Date
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Query
`````

You might also want to analyze the behavior of a particular host on a
particular day, perhaps for analyzing suspicious behavior by a
particular IP address. In that case, you'd write a query such as:

.. code-block:: pycon

   >>> q_events = db.events.find({
   ...     'host': '127.0.0.1',
   ...     'time': {'$gte':datetime(2000,10,10),'$lt':datetime(2000,10,11)}
   ... })

Index Support
`````````````

Once again, your choice of indexes affects the performance
characteristics of this query significantly. For instance, suppose you
create a compound index on (time, host):

.. code-block:: pycon

   >>> db.events.ensure_index([('time', 1), ('host', 1)])

In this case, the query plan would be the following (retrieved via
``q_events.explain()``):

.. code-block:: pycon

   { ...
     u'cursor': u'BtreeCursor time_1_host_1',
     u'indexBounds': {u'host': [[u'127.0.0.1', u'127.0.0.1']],
     u'time': [
         [ datetime.datetime(2000, 10, 10, 0, 0),
           datetime.datetime(2000, 10, 11, 0, 0)]]
     },
     ...
     u'millis': 4,
     u'n': 11,
     u'nscanned': 1296,
     u'nscannedObjects': 11,
     ... }

If, however, you create a compound index on (host, time)...

.. code-block:: pycon

   >>> db.events.ensure_index([('host', 1), ('time', 1)])

then get a much more efficient query plan and much better performance:

.. code-block:: pycon

   { ...
     u'cursor': u'BtreeCursor host_1_time_1',
     u'indexBounds': {u'host': [[u'127.0.0.1', u'127.0.0.1']],
     u'time': [[datetime.datetime(2000, 10, 10, 0, 0),
         datetime.datetime(2000, 10, 11, 0, 0)]]},
     ...
     u'millis': 0,
     u'n': 11,
     ...
     u'nscanned': 11,
     u'nscannedObjects': 11,
     ...
   }

In this case, MongoDB is able to visit just 11 entries in the index to
satisfy the query, whereas in the first it needed to visit 1296 entries.
This is because the query using (host, time) needs to search the index
range from ``('127.0.0.1', datetime(2000,10,10))`` to
``('127.0.0.1', datetime(2000,10,11))`` to satisfy the above query, whereas if you
used (time, host), the index range would be ``(datetime(2000,10,10), MIN_KEY)``
to ``(datetime(2000,10,10), MAX_KEY)``, a much larger range (in this case,
1296 entries) which will yield a correspondingly slower performance.

Although the index order has an impact on the performance of the query,
one thing to keep in mind is that an index scan is still *much* faster than a
collection scan. So using a (time, host) index would still be much
faster than an index on (time) alone. There is also the issue of
right-alignedness to consider, as the (time, host) index will be
right-aligned but the (host, time) index will not, and it's possible
that the right-alignedness of a (time, host) index will make up for the
increased number of index entries that need to be visited to satisfy
this query.

Counting the Number of Requests by Day and Page
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Aggregation
```````````

MongoDB 2.1 introduced a new :term:`aggregation framework` that allows
you to perform queries that aggregate large numbers of documents
significantly faster than the old :dbcommand:`mapreduce` and
:dbcommand:`group` :term:`commands <database commands>` in earlier
versions of MongoDB.

Suppose you'd like to find out how many requests there were for
each day and page over the last month, for instance. In this case, you
could build up the following aggregation pipeline:

.. code-block:: pycon

   >>> result = db.command('aggregate', 'events', pipeline=[
   ...         {  '$match': {
   ...               'time': {
   ...                   '$gte': datetime(2000,10,1),
   ...                   '$lt':  datetime(2000,11,1) } } },
   ...         {  '$project': {
   ...                 'path': 1,
   ...                 'date': {
   ...                     'y': { '$year': '$time' },
   ...                     'm': { '$month': '$time' },
   ...                     'd': { '$dayOfMonth': '$time' } } } },
   ...         { '$group': {
   ...                 '_id': {
   ...                     'p':'$path',
   ...                     'y': '$date.y',
   ...                     'm': '$date.m',
   ...                     'd': '$date.d' },
   ...                 'hits': { '$sum': 1 } } },
   ...         ])

The performance of this aggregation is dependent, of course, on your
choice of shard key if we're sharding. What you'd like to ensure is that
all the items in a particular 'group' are on the same server, which you
can do by sharding on date (probably not wise, as discussed below) or
path (possibly a good idea).

Index Support
`````````````

In this case, you want to make sure you have an index on the initial
$match query:

.. code-block:: pycon

   >>> db.events.ensure_index('time')

If you already have an index on (time, host) as discussed above,
however, there is no need to create a separate index on 'time' alone,
since the (time, host) index can be used to satisfy range queries on
'time' alone.

Sharding
--------

Your insertion rate is going to be limited by the number of shards you
maintain in your cluster as well as by the choice of a shard key. The
choice of a shard key is important because MongoDB uses *range-based
sharding* . What you *want* to happen is for the insertions to be
balanced equally among the shards, so you'd like to avoid using something
like a timestamp, sequence number, or ``ObjectId`` as a shard key, as new
inserts would tend to cluster around the same values (and thus the same
shard). But what you also *want* to happen is for each of your queries to
be routed to a single shard. The following are the pros and cons of each
approach.

Option 1: Shard by Time
~~~~~~~~~~~~~~~~~~~~~~~

Although an ``ObjectId`` or timestamp might seem to be an attractive
sharding key at first, particularly given the right-alignedness of the
index, it turns out to provide the worst of all worlds when it comes to
read and write performance. In this case, all of your inserts will always
flow to the same shard, providing no performance benefit write-side from
sharding. Your reads will also tend to cluster in the same shard, so you
would get no performance benefit read-side either.

Option 2: Shard by a Semi-Random Key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Suppose instead that you decided to shard on a key with a random
distribution, say the md5 or sha1 hash of the ``_id`` field:

.. code-block:: pycon

   >>> from bson import Binary
   >>> from hashlib import sha1
   >>>
   >>> # Introduce the synthetic shard key (this should actually be done at
   >>> #     event insertion time)
   >>>
   >>> for ev in db.events.find({}, {'_id':1}):
   ...     ssk = Binary(sha1(str(ev._id))).digest())
   ...     db.events.update({'_id':ev['_id']}, {'$set': {'ssk': ssk} })
   ...
   >>> db.command('shardcollection', 'events', {
   ...     key : { 'ssk' : 1 } })
   { "collectionsharded" : "events", "ok" : 1 }

This does introduce some complexity into your application in order to
generate the random key, but it provides you linear scaling on your
inserts, so 5 shards should yield a 5x speedup in inserting. The
downsides to using a random shard key are the following: a) the shard
key's index will tend to take up more space (and you need an index to
determine where to place each new insert), and b) queries (unless they
include the synthetic, random-ish shard key) will need to be distributed
to all your shards in parallel. This may be acceptable, since in this
scenario write performance is much more important than  read
performance, but you should be aware of the downsides to using a random
key distribution.

Option 3: Shard by an Evenly-Distributed Key in the Data Set
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this case, you might choose to shard on the 'path' attribute, since it
seems to be relatively evenly distributed:

.. code-block:: pycon

   >>> db.command('shardcollection', 'events', {
   ...     key : { 'path' : 1 } })
   { "collectionsharded" : "events", "ok" : 1 }

This has a couple of advantages: a) writes tend to be evenly balanced,
and b) reads tend to be selective (assuming they include the 'path'
attribute in the query). There is a potential downside to this approach,
however, particularly in the case where there are a limited number of
distinct values for the path. In that case, you can end up with large
shard 'chunks' that cannot be split or rebalanced because they contain
only a single shard key. The rule of thumb here is that you should not
pick a shard key which allows large numbers of documents to have the
same shard key since this prevents rebalancing.

Option 4: Shard by Combine a Natural and Synthetic Key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This approach is perhaps the best combination of read and write
performance for the application. You can define the shard key to be
(path, sha1(\_id)):

.. code-block:: pycon

   >>> db.command('shardcollection', 'events', {
   ...     key : { 'path' : 1, 'ssk': 1 } })
   { "collectionsharded" : "events", "ok" : 1 }

You still need to calculate a synthetic key gin the application client,
but in return you get good write balancing as well as good read
selectivity.

Test with Your Own Data
~~~~~~~~~~~~~~~~~~~~~~~

Picking a good shard key is unfortunately still one of those decisions
that is simultaneously difficult to make, high-impact, and difficult to
change once made. The particular mix of reading and writing, as well as
the particular queries used, all have a large impact on the performance
of different sharding configurations. Although you can choose a
reasonable shard key based on the considerations above, the best
approach is to analyze the actual insertions and queries you are using
in your own application.

.. _rta-storing-log-data-capped-collections: 

Capped Collections
------------------

One variation that you may want to consider based on your data retention
requirements is whether you might be able to use a `capped
collection <http://www.mongodb.org/display/DOCS/Capped+Collections>`_ to
store your events. Capped collections might be a good choice if you know
you will process the event documents in a timely manner and you don't
have exacting data retention requirements on the event data. Capped
collections have the advantage of never growing beyond a certain size
(they are allocated as a circular buffer on the disk) and having
documents 'fall out' of the buffer in their insertion order. Uncapped
collections (the default) will persist documents until they are
explicitly removed from the collection or the collection is dropped.

Managing Event Data Growth
--------------------------

MongoDB databases, in the course of normal operation, never relinquish
disk space back to the file system. This can create difficulties if you
don't manage the size of your databases up front. For event data, you
have a few options for managing the data growth:

Single Collection
~~~~~~~~~~~~~~~~~

This is the simplest option: keep all events in a single collection,
periodically removing documents that you don't need any more. The
advantage of simplicity, however, is offset by some performance
considerations. First, when you execute your remove, MongoDB will actually
bring the documents being removed into memory. Since these are documents
that presumably you haven't touched in a while (that's why you're deleting
them), this will force more relevant data to be flushed out to disk.
Second, in order to do a reasonably fast remove operation, you probably
want to keep an index on a timestamp field. This will tend to slow down
your inserts, as the inserts have to update the index as well as write
the event data. Finally, removing data periodically will also be the
option that has the most potential for fragmenting the database, as
MongoDB attempts to reuse the space freed by the remove operations for
new events.

Multiple Collections, Single Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The next option is to periodically *rename* your event collection,
rotating collections in much the same way you might rotate log files. You
would then drop the oldest collection from the database. This has
several advantages over the single collection approach. First off,
collection renames are both fast and atomic. Secondly, you don't actually
have to touch any of the documents to drop a collection. Finally, since
MongoDB allocates storage in *extents* that are owned by collections,
dropping a collection will free up entire extents, mitigating the
fragmentation risk. The downside to using multiple collections is
increased complexity, since you will probably need to query both the
current event collection and the previous event collection for any data
analysis you perform.

Multiple Databases
~~~~~~~~~~~~~~~~~~

In the multiple database option, you take the multiple collection option
a step further. Now, rather than rotating your collections, you will
rotate your databases. At the cost of rather increased complexity both in
insertions and queries, you do gain one benefit: as your databases get
dropped, disk space gets returned to the operating system. This option
would only really make sense if you had extremely variable event
insertion rates or if you had variable data retention requirements. For
instance, if you are performing a large backfill of event data and want
to make sure that the entire set of event data for 90 days is available
during the backfill, but can be reduced to 30 days in ongoing
operations, you might consider using multiple databases. The complexity
cost for multiple databases, however, is significant, so this option
should only be taken after thorough analysis.
