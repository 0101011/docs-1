Real Time Analytics: Storing Log Data
=====================================

Problem
-------

You have one or more servers generating events that you would like to
persist to a MongoDB collection.

Solution overview
-----------------

For this solution, we will assume that each server generating events has
access to the MongoDB server(s). We will also assume that the consumer
of the event data has access to the MongoDB server(s) and that the query
rate is (substantially) lower than the insert rate (as is most often the
case when logging a high-bandwidth event stream).

Schema design
-------------

\*\*
The schema design in this case will depend largely on the particular
format of the event data you want to store. For a simple example, let's
take standard request logs from the Apache web server using the combined
log format. For this example we will assume you're using an uncapped
collection to store the event data. A line from such a log file might
look like the following:

``127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326 "``[http://www.example.com/start.html](http://www.example.com/start.html)``" "Mozilla/4.08 [en] (Win98; I ;Nav)"``

\`

The simplest approach to storing the log data would be putting the exact
text of the log record into a document:

``{``

``_id: ObjectId('4f442120eb03305789000000'),``

``line: '127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326 "``[http://www.example.com/start.html](http://www.example.com/start.html)``" "Mozilla/4.08 [en] (Win98; I ;Nav)"'``

``}``

While this is a possible solution, it's not likely to be the optimal
solution. For instance, if we decided we wanted to find events that hit
the same page, we would need to use a regular expression query, which
would require a full collection scan. A better approach would be to
extract the relevant fields into individual properties. When doing the
extraction, we should pay attention to the choice of data types for the
various fields. For instance, the date field in the log line
``[10/Oct/2000:13:55:36 -0700]``is 28 bytes long. If we instead store
this as a UTC timestamp, it shrinks to 8 bytes. Storing the date as a
timestamp also gives us the advantage of being able to make date range
queries, whereas comparing two date *strings* is nearly useless. A
similar argument applies to numeric fields; storing them as strings is
suboptimal, taking up more space and making the appropriate types of
queries much more difficult.

We should also consider what information we might want to omit from the
log record. For instance, if we wanted to record exactly what was in the
log record, we might create a document like the following:

``{``

``_id: ObjectId('4f442120eb03305789000000'),``

``host: "127.0.0.1",``

``logname: null,``

``user: 'frank',``

``time:  ,``

``request: "GET /apache_pb.gif HTTP/1.0",``

``status: 200,``

``response_size: 2326,``

``referer: "``[http://www.example.com/start.html](http://www.example.com/start.html)``",``

``user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"``

``}``

\`

In most cases, however, we probably are only interested in a subset of
the data about the request. Here, we may want to keep the host, time,
path, user agent, and referer for a web analytics application:

``{``

``_id: ObjectId('4f442120eb03305789000000'),``

``host: "127.0.0.1",``

``time:  ISODate("2000-10-10T20:55:36Z"),``

``path: "/apache_pb.gif",``

``referer: "``[http://www.example.com/start.html](http://www.example.com/start.html)``",``

``user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"``

``}``

\`

It might even be possible to remove the time, since ObjectIds embed
their the time they are created:

``{``

``_id: ObjectId('4f442120eb03305789000000'),``

``host: "127.0.0.1",``

``time:  ISODate("2000-10-10T20:55:36Z"),``

``path: "/apache_pb.gif",``

``referer: "``[http://www.example.com/start.html](http://www.example.com/start.html)``",``

``user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"``

``}``

\`

**System Architecture**

\*\*
For an event logging system, we are mainly concerned with two
performance considerations: 1) how many inserts per second can we
perform (this will limit our event throughput) and 2) how will we manage
the growth of event data. Concerning insert performance, the best way to
scale the architecture is via sharding.

Operations
----------

The main performance-critical operation we're concerned with in storing
an event log is the insertion speed. However, we also need to be able to
query the event data for relevant statistics. This section will describe
each of these operations, using the Python programming language and the
pymongo MongoDB driver. These operations would be similar in other
languages as well.

Inserting a log record
~~~~~~~~~~~~~~~~~~~~~~

In many event logging applications, we can accept some degree of risk
when it comes to dropping events. In others, we need to be absolutely
sure we don't drop any events. MongoDB supports both models. In the case
where we can tolerate a risk of loss, we can insert records
*asynchronously* using a fire- and-forget model:

``>>> import bson``

``>>> import pymongo``

``>>> from datetime import datetime``

``>>> conn = pymongo.Connection()``

``>>> db = conn.event_db``

``>>> event = {``

``...     _id: bson.ObjectId(),``

``...     host: "127.0.0.1",``

``...     time:  datetime(2000,10,10,20,55,36),``

``...     path: "/apache_pb.gif",``

``...     referer: "``[http://www.example.com/start.html](http://www.example.com/start.html)``",``

``...     user_agent: "Mozilla/4.08 [en] (Win98; I ;Nav)"``

``...}``

``>>> db.events.insert(event, safe=False)``

This is the fastest approach, as our code doesn't even require a
round-trip to the MongoDB server to ensure that the insert was received.
It is thus also the riskiest approach, as we will not detect network
failures nor server errors (such as DuplicateKeyErrors on a unique
index). If we want to make sure we have an acknowledgement from the
server that our insertion succeeded (for some definition of success), we
can pass safe=True:

``>>> db.events.insert(event, safe=True)``

\`

If our tolerance for data loss risk is somewhat less, we can require
that the server to which we write the data has committed the event to
the on-disk journal before we continue operation (``safe=True`` is
implied by all the following options):

``>>> db.events.insert(event, j=True)``

\`

Finally, if we have *extremely low* tolerance for event data loss, we
can require the data to be replicated to multiple secondary servers
before returning:

``>>> db.events.insert(event, w=2)``

In this case, we have requested acknowledgement that the data has been
replicated to 2 replicas. We can combine options as well:

``>>> db.events.insert(event, j=True, w=2)``

In this case, we are waiting on both a journal commit *and* a
replication acknowledgement. Although this is the safest option, it is
also the slowest, so you should be aware of the trade-off when
performing your inserts.

Aside: Bulk Inserts
^^^^^^^^^^^^^^^^^^^

If at all possible in our application architecture, we should consider
using bulk inserts to insert event data. All the options discussed above
apply to bulk inserts, but you can actually pass multiple events as the
first parameter to .insert(). By passing multiple documents into a
single insert() call, we are able to amortize the performance penalty we
incur by using the 'safe' options such as j=True or w=2.

Finding all the events for a particular page
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For a web analytics-type operation, getting the logs for a particular
web page might be a common operation that we would want to optimize for.
In this case, the query would be as follows:

``>>> q_events = db.events.find({'path': '/apache_pb.gif'})``

\`

Note that the sharding setup we use (should we decide to shard this
collection) has performance implications for this operation. For
instance, if we shard on the 'path' property, then this query will be
handled by a single shard, whereas if we shard on some other property or
combination of properties, the mongos instance will be forced to do a
scatter/gather operation which involves *all* the shards.

Index support
^^^^^^^^^^^^^

This operation would benefit significantly from an index on the 'path'
attribute:

\`

``>>> db.events.ensure_index('path')``

\`

One potential downside to this index is that it is relatively randomly
distributed, meaning that for efficient operation the entire index
should be resident in RAM. Since there is likely to be a relatively
small number of distinct paths in the index, however, this will probably
not be a problem.

Finding all the events for a particular date
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We may also want to find all the events for a particular date. In this
case, we would perform the following query:

``>>> q_events = db.events.find('time':``

``...     { '$gte':datetime(2000,10,10),'$lt':datetime(2000,10,11)})``

\`

Index support
^^^^^^^^^^^^^

In this case, an index on 'time' would provide optimal performance:

\`

``>>> db.events.ensure_index('time')``

One of the nice things about this index is that it is *right-aligned.*
Since we are always inserting events in ascending time order, the
right-most slice of the B-tree will always be resident in RAM. So long
as our queries focus mainly on recent events, the *only* part of the
index that needs to be resident in RAM is the right-most slice of the
B-tree, allowing us to keep quite a large index without using up much of
our system memory.

Finding all the events for a particular host/date
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\`

We might also want to analyze the behavior of a particular host on a
particular day, perhaps for analyzing suspicious behavior by a
particular IP address. In that case, we would write a query such as:

``>>> q_events = db.events.find({``

``...     'host': '127.0.0.1',``

``...     'time': {'$gte':datetime(2000,10,10),'$lt':datetime(2000,10,11)}``

``... })``

Index support
^^^^^^^^^^^^^

Once again, our choice of indexes affects the performance
characteristics of this query significantly. For instance, suppose we
create a compound index on (time, host):

``>>> db.events.ensure_index([('time', 1), ('host', 1)])``

In this case, the query plan would be the following (retrieved via
q\_events.explain()):

``{``

``…``

``u'cursor': u'BtreeCursor time_1_host_1',``

``u'indexBounds': {u'host': [[u'127.0.0.1', u'127.0.0.1']],``

``u'time': [[datetime.datetime(2000, 10, 10, 0, 0),``

``datetime.datetime(2000, 10, 11, 0, 0)]]},``

``…``

``u'millis': 4,``

``u'n': 11,``

``…``

``u'nscanned': 1296,``

``u'nscannedObjects': 11,``

``…``

``}``

\`

If, however, we create a compound index on (host, time)...

\`

``>>> db.events.ensure_index([('host', 1), ('time', 1)])``

\`

We get a much more efficient query plan and much better performance:

\`

``{``

``…``

``u'cursor': u'BtreeCursor host_1_time_1',``

``u'indexBounds': {u'host': [[u'127.0.0.1', u'127.0.0.1']],``

``u'time': [[datetime.datetime(2000, 10, 10, 0, 0),``

``datetime.datetime(2000, 10, 11, 0, 0)]]},``

``…``

``u'millis': 0,``

``u'n': 11,``

``…``

``u'nscanned': 11,``

``u'nscannedObjects': 11,``

``…``

``}``

\`

In this case, MongoDB is able to visit just 11 entries in the index to
satisfy the query, whereas in the first it needed to visit 1296 entries.
This is because the query using (host, time) needs to search the index
range from ('127.0.0.1', datetime(2000,10,10)) to ('127.0.0.1',
datetime(2000,10,11)) to satisfy the above query, whereas if we used
(time, host), the index range would be (datetime(2000,10,10), MIN\_KEY)
to (datetime(2000,10,10), MAX\_KEY), a much larger range (in this case,
1296 entries) which will yield a correspondingly slower performance.

Although the index order has an impact on the performance of the query,
one thing to keep in mind is that an index scan is *much* faster than a
collection scan. So using a (time, host) index would still be much
faster than an index on (time) alone. There is also the issue of
right-alignedness to consider, as the (time, host) index will be
right-aligned but the (host, time) index will not, and it's possible
that the right-alignedness of a (time, host) index will make up for the
increased number of index entries that need to be visited to satisfy
this query.

Counting the number of requests by day and page
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

MongoDB 2.1 introduced a new aggregation framework that allows us to
perform queries that aggregate large numbers of documents significantly
faster than the old 'mapreduce' and 'group' commands in prior versions
of MongoDB. Suppose we want to find out how many requests there were for
each day and page over the last month, for instance. In this case, we
could build up the following aggregation pipeline:

\`

``>>> result = db.command('aggregate', 'events', pipeline=[``

``...         {  '$match': {``

``...               'time': {``

``...                   '$gte': datetime(2000,10,1),``

``...                   '$lt':  datetime(2000,11,1) } } },``

``...         {  '$project': {``

``...                 'path': 1,``

``...                 'date': {``

``...                     'y': { '$year': '$time' },``

``...                     'm': { '$month': '$time' },``

``...                     'd': { '$dayOfMonth': '$time' } } } },``

``...         { '$group': {``

``...                 '_id': {``

``...                     'p':'$path',``

``...                     'y': '$date.y',``

``...                     'm': '$date.m',``

``...                     'd': '$date.d' },``

``...                 'hits': { '$sum': 1 } } },``

``...         ])``

\`

The performance of this aggregation is dependent, of course, on our
choice of shard key if we're sharding. What we'd like to ensure is that
all the items in a particular 'group' are on the same server, which we
can do by sharding on date (probably not wise, as we discuss below) or
path (possibly a good idea).

Index support
^^^^^^^^^^^^^

In this case, we want to make sure we have an index on the initial
$match query:

\`

``>>> db.events.ensure_index('time')``

\`

If we already have an index on ('time', 'host') as discussed above,
however, there is no need to create a separate index on 'time' alone,
since the ('time', 'host') index can be used to satisfy range queries on
'time' alone.

Sharding
--------

Our insertion rate is going to be limited by the number of shards we
maintain in our cluster as well as by our choice of a shard key. The
choice of a shard key is important because MongoDB uses *range-based
sharding* . What we *want* to happen is for the insertions to be
balanced equally among the shards, so we want to avoid using something
like a timestamp, sequence number, or ObjectId as a shard key, as new
inserts would tend to cluster around the same values (and thus the same
shard). But what we also *want* to happen is for each of our queries to
be routed to a single shard. Here, we discuss the pros and cons of each
approach.

Option 0: Shard on time
~~~~~~~~~~~~~~~~~~~~~~~

Although an ObjectId or timestamp might seem to be an attractive
sharding key at first, particularly given the right-alignedness of the
index, it turns out to provide the worst of all worlds when it comes to
read and write performance. In this case, all of our inserts will always
flow to the same shard, providing no performance benefit write-side from
sharding. Our reads will also tend to cluster in the same shard, so we
would get no performance benefit read-side either.

Option 1: Shard on a random(ish) key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Supose instead that we decided to shard on a key with a random
distribution, say the md5 or sha1 hash of the '\_id' field:

\`

``>>> from bson import Binary``

``>>> from hashlib import sha1``

``>>>``

``>>> # Introduce the synthetic shard key (this should actually be done at``

``>>> #     event insertion time)``

``>>>``

``>>> for ev in db.events.find({}, {'_id':1}):``

``...     ssk = Binary(sha1(str(ev._id))).digest())``

``...     db.events.update({'_id':ev['_id']}, {'$set': {'ssk': ssk} })``

``...``

``>>> db.command('shardcollection', 'events', {``

``...     key : { 'ssk' : 1 } })``

``{ "collectionsharded" : "events", "ok" : 1 }``

This does introduce some complexity into our application in order to
generate the random key, but it provides us linear scaling on our
inserts, so 5 shards should yield a 5x speedup in inserting. The
downsides to using a random shard key are the following: a) the shard
key's index will tend to take up more space (and we need an index to
determine where to place each new insert), and b) queries (unless they
include our synthetic, random-ish shard key) will need to be distributed
to all our shards in parallel. This may be acceptable, since in our
scenario our write performance is much more important than our read
performance, but we should be aware of the downsides to using a random
key distribution.

Option 2: Shard on a naturally evenly-distributed key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this case, we might choose to shard on the 'path' attribute, since it
seems to be relatively evenly distributed:

``>>> db.command('shardcollection', 'events', {``

``...     key : { 'path' : 1 } })``

``{ "collectionsharded" : "events", "ok" : 1 }``

This has a couple of advantages: a) writes tend to be evenly balanced,
and b) reads tend to be selective (assuming they include the 'path'
attribute in the query). There is a potential downside to this approach,
however, particularly in the case where there are a limited number of
distinct values for the path. In that case, you can end up with large
shard 'chunks' that cannot be split or rebalanced because they contain
only a single shard key. The rule of thumb here is that we should not
pick a shard key which allows large numbers of documents to have the
same shard key since this prevents rebalancing.

Option 3: Combine a natural and synthetic key
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This approach is perhaps the best combination of read and write
performance for our application. We can define the shard key to be
(path, sha1(\_id)):

\`

``>>> db.command('shardcollection', 'events', {``

``...     key : { 'path' : 1, 'ssk': 1 } })``

``{ "collectionsharded" : "events", "ok" : 1 }``

We still need to calculate a synthetic key in the application client,
but in return we get good write balancing as well as good read
selectivity.

Sharding conclusion: Test with your own data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Picking a good shard key is unfortunately still one of those decisions
that is simultaneously difficult to make, high-impact, and difficult to
change once made. The particular mix of reading and writing, as well as
the particular queries used, all have a large impact on the performance
of different sharding configurations. Although you can choose a
reasonable shard key based on the considerations above, the best
approach is to analyze the actual insertions and queries you are using
in your own application.

Variation: Capped Collections
-----------------------------

One variation that you may want to consider based on your data retention
requirements is whether you might be able to use a `capped
collection <http://www.mongodb.org/display/DOCS/Capped+Collections>`_ to
store your events. Capped collections might be a good choice if you know
you will process the event documents in a timely manner and you don't
have exacting data retention requirements on the event data. Capped
collections have the advantage of never growing beyond a certain size
(they are allocated as a circular buffer on the disk) and having
documents 'fall out' of the buffer in their insertion order. Uncapped
collections (the default) will persist documents until they are
explicitly removed from the collection or the collection is dropped.

Appendix: Managing Event Data Growth
------------------------------------

MongoDB databases, in the course of normal operation, never relinquish
disk space back to the file system. This can create difficulties if you
don't manage the size of your databases up front. For event data, we
have a few options for managing the data growth:

Single Collection
~~~~~~~~~~~~~~~~~

This is the simplest option: keep all events in a single collection,
periodically removing documents that we don't need any more. The
advantage of simplicity, however, is offset by some performance
considerations. First, when we execute our remove, MongoDB will actually
bring the documents being removed into memory. Since these are documents
that presumably we haven't touched in a while (that's why we're deleting
them), this will force more relevant data to be flushed out to disk.
Second, in order to do a reasonably fast remove operation, we probably
want to keep an index on a timestamp field. This will tend to slow down
our inserts, as the inserts have to update the index as well as write
the event data. Finally, removing data periodically will also be the
option that has the most potential for fragmenting the database, as
MongoDB attempts to reuse the space freed by the remove operations for
new events.

Multiple Collections, Single Database
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Our next option is to periodically *rename* our event collection,
rotating collections in much the same way we might rotate log files. We
would then drop the oldest collection from the database. This has
several advantages over the single collection approach. First off,
collection renames are both fast and atomic. Secondly, we don't actually
have to touch any of the documents to drop a collection. Finally, since
MongoDB allocates storage in *extents* that are owned by collections,
dropping a collection will free up entire extents, mitigating the
fragmentation risk. The downside to using multiple collections is
increased complexity, since you will probably need to query both the
current event collection and the previous event collection for any data
analysis you perform.

Multiple Databases
~~~~~~~~~~~~~~~~~~

In the multiple database option, we take the multiple collection option
a step further. Now, rather than rotating our collections, we will
rotate our databases. At the cost of rather increased complexity both in
insertions and queries, we do gain one benefit: as our databases get
dropped, disk space gets returned to the operating system. This option
would only really make sense if you had extremely variable event
insertion rates or if you had variable data retention requirements. For
instance, if you are performing a large backfill of event data and want
to make sure that the entire set of event data for 90 days is available
during the backfill, but can be reduced to 30 days in ongoing
operations, you might consider using multiple databases. The complexity
cost for multiple databases, however, is significant, so this option
should only be taken after thorough analysis.

Page of
