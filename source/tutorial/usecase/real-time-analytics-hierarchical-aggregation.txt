Real Time Analytics: Hierarchical Aggregation
=============================================

Problem
-------

You have a large amount of event data that you want to analyze at
multiple levels of aggregation.

Solution overview
-----------------

For this solution we will assume that the incoming event data is already
stored in an incoming 'events' collection. For details on how we could
get the event data into the events collection, please see "Real Time
Analytics: Storing Log Data."

Once the event data is in the events collection, we need to aggregate
event data to the finest time granularity we're interested in. Once that
data is aggregated, we will use it to aggregate up to the next level of
the hierarchy, and so on. To perform the aggregations, we will use
MongoDB's mapreduce command. Our schema will use several collections:
the raw data (event) logs and collections for statistics aggregated
hourly, daily, weekly, monthly, and yearly. We will use a hierarchical
approach to running our map-reduce jobs. The input and output of each
job is illustrated below:

.. figure:: https://docs.google.com/a/arborian.com/drawings/image?id=syuQgkoNVdeOo7UC4WepaPQ&rev=1&h=208&w=268&ac=1
   :align: center
   :alt: Hierarchy

   Hierarchy
Note that the events rolling into the hourly collection is qualitatively
different than the hourly statistics rolling into the daily collection.

Aside: Map-Reduce Algorithm
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Map/reduce is a popular aggregation algorithm that is optimized for
embarrassingly parallel problems. The psuedocode (in Python) of the
map/reduce algorithm appears below. Note that we are providing the
psuedocode for a particular type of map/reduce where the results of the
map/reduce operation are *reduced* into the result collection, allowing
us to perform incremental aggregation which we'll need in this case.

::

    def map_reduce(icollection, query,
        mapf, reducef, finalizef, ocollection):
        '''Psuedocode for map/reduce with output type="reduce" in MongoDB'''
        map_results = defaultdict(list)
        def emit(key, value):
            '''helper function used inside mapf'''
            map_results[key].append(value)


        # The map phase
        for doc in icollection.find(query):
            mapf(doc)


        # Pull in documents from the output collection for
        # output type='reduce'
        for doc in ocollection.find({'_id': {'$in': map_results.keys() } }):
            map_results[doc['_id']].append(doc['value'])


        # The reduce phase
        for key, values in map_results.items():
            reduce_results[key] = reducef(key, values)


        # Finalize and save the results back
        for key, value in reduce_results.items():
            final_value = finalizef(key, value)
            ocollection.save({'_id': key, 'value': final_value})

The embarrassingly parallel part of the map/reduce algorithm lies in the
fact that each invocation of mapf, reducef, and finalizef are
independent of each other and can, in fact, be distributed to different
servers. In the case of MongoDB, this parallelism can be achieved by
using sharding on the collection on which we are performing map/reduce.

Schema design
-------------

When designing the schema for event storage, we need to keep in mind the
necessity to differentiate between events which have been included in
our aggregations and events which have not yet been included. A simple
approach in a relational database would be to use an auto-increment
integer primary key, but this introduces a big performance penalty to
our event logging process as it has to fetch event keys one-by one.

If we are able to batch up our inserts into the event table, we can
still use an auto-increment primary key by using the find\_and\_modify
command to generate our \_id values:

::

    >>> obj = db.my_sequence.find_and_modify(
    ...     query={'_id':0},
    ...     update={'$inc': {'inc': 50}}
    ...     upsert=True,
    ...     new=True)
    >>> batch_of_ids = range(obj['inc']-50, obj['inc'])

In most cases, however, it is sufficient to include a timestamp with
each event that we can use as a marker of which events have been
processed and which ones remain to be processed. For this use case,
we'll assume that we are calculating average session length for
logged-in users on a website. Our event format will thus be the
following:

::

    {
        "userid": "rick",
        "ts": ISODate('2010-10-10T14:17:22Z'),
        "length":95
    }

We want to calculate total and average session times for each user at
the hour, day, week, month, and year. In each case, we will also store
the number of sessions to enable us to incrementally recompute the
average session times. Each of our aggregate documents, then, looks like
the following:

::

    {
       _id: { u: "rick", d: ISODate("2010-10-10T14:00:00Z") },
       value: {
           ts: ISODate('2010-10-10T15:01:00Z'),
           total: 254,
           count: 10,
           mean: 25.4 }
    }

Note in particular that we have added a timestamp to the aggregate
document. This will help us as we incrementally update the various
levels of the hierarchy.

Operations
----------

In the discussion below, we will assume that all the events have been
inserted and appropriately timestamped, so our main operations are
aggregating from events into the smallest aggregate (the hourly totals)
and aggregating from smaller granularity to larger granularity. In each
case, we will assume that the last time the particular aggregation was
run is stored in a last\_run variable. (This variable might be loaded
from MongoDB or another persistence mechanism.)

Aggregate from events to the hourly level
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Here, we want to load all the events since our last run until one minute
ago (to allow for some lag in logging events). The first thing we need
to do is create our map function. Even though we will be using Python
and PyMongo to interface with the MongoDB server, note that the various
functions (map, reduce, and finalize) that we pass to the mapreduce
command must be Javascript functions. The map function appears below:

::

    mapf_hour = bson.Code('''function() {
        var key = {
            u: this.userid,
            d: new Date(
                this.ts.getFullYear(),
                this.ts.getMonth(),
                this.ts.getDate(),
                this.ts.getHours(),
                0, 0, 0);
        emit(
            key,
            {
                total: this.length,
                count: 1,
                mean: 0,
                ts: new Date(); });
    }''')

In this case, we are emitting key, value pairs which contain the
statistics we want to aggregate as you'd expect, but we are also
emitting 'ts' value. This will be used in the cascaded aggregations
(hour to day, etc.) to determine when a particular hourly aggregation
was performed.

Our reduce function is also fairly straightforward:

::

    reducef = bson.Code('''function(key, values) {
        var r = { total: 0, count: 0, mean: 0, ts: null };
        values.forEach(function(v) {
            r.total += v.total;
            r.count += v.count;
        });
        return r;
    }''')

A few things are notable here. First of all, note that the returned
document from our reduce function has the same format as the result of
our map. This is a characteristic of our map/reduce that we would like
to maintain, as differences in structure between map, reduce, and
finalize results can lead to difficult-to-debug errors. Also note that
we are ignoring the 'mean' and 'ts' values. These will be provided in
the 'finalize' step:

::

    finalizef = bson.Code('''function(key, value) {
        if(value.count > 0) {
            value.mean = value.total / value.count;
        }
        value.ts = new Date();
        return value;
    }''')

Here, we compute the mean value as well as the timestamp we will use to
write back to the output collection. Now, to bind it all together, here
is our Python code to invoke the mapreduce command:

::

    cutoff = datetime.utcnow() - timedelta(seconds=60)
    query = { 'ts': { '$gt': last_run, '$lt': cutoff } }


    db.events.map_reduce(
        map=mapf_hour,
        reduce=reducef,
        finalize=finalizef,
        query=query,
        out={ 'reduce': 'stats.hourly' })


    last_run = cutoff

Because we used the 'reduce' option on our output, we are able to run
this aggregation as often as we like as long as we update the last\_run
variable.

Index support
^^^^^^^^^^^^^

Since we are going to be running the initial query on the input events
frequently, we would benefit significantly from and index on the
timestamp of incoming events:

::

    >>> db.stats.hourly.ensure_index('ts')

Since we are always reading and writing the most recent events, this
index has the advantage of being right-aligned, which basically means we
only need a thin slice of the index (the most recent values) in RAM to
achieve good performance.

Aggregate from hour to day
~~~~~~~~~~~~~~~~~~~~~~~~~~

In calculating the daily statistics, we will use the hourly statistics
as input. Our map function looks quite similar to our hourly map
function:

::

    mapf_day = bson.Code('''function() {
        var key = {
            u: this._id.u,
            d: new Date(
                this._id.d.getFullYear(),
                this._id.d.getMonth(),
                this._id.d.getDate(),
                0, 0, 0, 0) };
        emit(
            key,
            {
                total: this.value.total,
                count: this.value.count,
                mean: 0,
                ts: null });
    }''')

There are a few differences to note here. First of all, the key to which
we aggregate is the (userid, date) rather than (userid, hour) to allow
for daily aggregation. Secondly, note that the keys and values we emit
are actually the total and count values from our hourly aggregates
rather than properties from event documents. This will be the case in
all our higher-level hierarchical aggregations.

Since we are using the same format for map output as we used in the
hourly aggregations, we can, in fact, use the same reduce and finalize
functions. The actual Python code driving this level of aggregation is
as follows:

::

    cutoff = datetime.utcnow() - timedelta(seconds=60)
    query = { 'value.ts': { '$gt': last_run, '$lt': cutoff } }


    db.stats.hourly.map_reduce(
        map=mapf_day,
        reduce=reducef,
        finalize=finalizef,
        query=query,
        out={ 'reduce': 'stats.daily' })


    last_run = cutoff

There are a couple of things to note here. First of all, our query is
not on 'ts' now, but 'value.ts', the timestamp we wrote during the
finalization of our hourly aggregates. Also note that we are, in fact,
aggregating from the stats.hourly collection into the stats.daily
collection.

Index support
^^^^^^^^^^^^^

Since we are going to be running the initial query on the hourly
statistics collection frequently, an index on 'value.ts' would be nice
to have:

::

    >>> db.stats.hourly.ensure_index('value.ts')

Once again, this is a right-aligned index that will use very little RAM
for efficient operation.
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Other aggregations
~~~~~~~~~~~~~~~~~~

Once we have our daily statistics, we can use them to calculate our
weekly and monthly statistics. Our weekly map function is as follows:

::

    mapf_week = bson.Code('''function() {
        var key = {
            u: this._id.u,
            d: new Date(
                this._id.d.valueOf()
                - dt.getDay()*24*60*60*1000) };
        emit(
            key,
            {
                total: this.value.total,
                count: this.value.count,
                mean: 0,
                ts: null });
    }''')

Here, in order to get our group key, we are simply taking the date and
subtracting days until we get to the beginning of the week. In our
weekly map function, we will choose the first day of the month as our
group key:

::

    mapf_month = bson.Code('''function() {
            d: new Date(
                this._id.d.getFullYear(),
                this._id.d.getMonth(),
                1, 0, 0, 0, 0) };
        emit(
            key,
            {
                total: this.value.total,
                count: this.value.count,
                mean: 0,
                ts: null });
    }''')

One thing in particular to notice about these map functions is that they
are identical to one another except for the date calculation. We can use
Python's string interpolation to refactor our map function definitions
as follows:

::

    mapf_hierarchical = '''function() {
        var key = {
            u: this._id.u,
            d: %s };
        emit(
            key,
            {
                total: this.value.total,
                count: this.value.count,
                mean: 0,
                ts: null });
    }'''


    mapf_day = bson.Code(
        mapf_hierarchical % '''new Date(
                this._id.d.getFullYear(),
                this._id.d.getMonth(),
                this._id.d.getDate(),
                0, 0, 0, 0)''')


    mapf_week = bson.Code(
        mapf_hierarchical % '''new Date(
                this._id.d.valueOf()
                - dt.getDay()*24*60*60*1000)''')


    mapf_month = bson.Code(
        mapf_hierarchical % '''new Date(
                this._id.d.getFullYear(),
                this._id.d.getMonth(),
                1, 0, 0, 0, 0)''')


    mapf_year = bson.Code(
        mapf_hierarchical % '''new Date(
                this._id.d.getFullYear(),
                1, 1, 0, 0, 0, 0)''')

Our Python driver can also be refactored so we have much less code
duplication:

::

    def aggregate(icollection, ocollection, mapf, cutoff, last_run):
        query = { 'value.ts': { '$gt': last_run, '$lt': cutoff } }
        icollection.map_reduce(
            map=mapf,
            reduce=reducef,
            finalize=finalizef,
            query=query,
            out={ 'reduce': ocollection.name })

Once this is defined, we can perform all our aggregations as follows:

::

    cutoff = datetime.utcnow() - timedelta(seconds=60)
    aggregate(db.events, db.stats.hourly, mapf_hour, cutoff, last_run)
    aggregate(db.stats.hourly, db.stats.daily, mapf_day, cutoff, last_run)
    aggregate(db.stats.daily, db.stats.weekly, mapf_week, cutoff, last_run)
    aggregate(db.stats.daily, db.stats.monthly, mapf_month, cutoff,
        last_run)
    aggregate(db.stats.monthly, db.stats.yearly, mapf_year, cutoff,
        last_run)
    last_run = cutoff

So long as we save/restore our 'last\_run' variable between
aggregations, we can run these aggregations as often as we like since
each aggregation individually is incremental.

Index support
^^^^^^^^^^^^^

Our indexes will continue to be on the value's timestamp to ensure
efficient operation of the next level of the aggregation (and they
continue to be right-aligned):

::

    >>> db.stats.daily.ensure_index('value.ts')
    >>> db.stats.monthly.ensure_index('value.ts')

Sharding
--------

To take advantage of distinct shards when performing map/reduce, our
input collections should be sharded. In order to achieve good balancing
between nodes, we should make sure that the shard key we use is not
simply the incoming timestamp, but rather something that varies
significantly in the most recent documents. In this case, the username
makes sense as the most significant part of the shard key.

In order to prevent a single, active user from creating a large,
unsplittable chunk, we will use a compound shard key with (username,
timestamp) on each of our collections: >>> db.command('shardcollection',
'events', { ... key : { 'userid': 1, 'ts' : 1} } ) { "collectionsharded"
: "events", "ok" : 1 } >>> db.command('shardcollection', 'stats.daily',
{ ... key : { '\_id': 1} } ) { "collectionsharded" : "stats.daily", "ok"
: 1 } >>> db.command('shardcollection', 'stats.weekly', { ... key : {
'\_id': 1} } ) { "collectionsharded" : "stats.weekly", "ok" : 1 } >>>
db.command('shardcollection', 'stats.monthly', { ... key : { '\_id': 1}
} ) { "collectionsharded" : "stats.monthly", "ok" : 1 } >>>
db.command('shardcollection', 'stats.yearly', { ... key : { '\_id': 1} }
) { "collectionsharded" : "stats.yearly", "ok" : 1 }

We should also update our map/reduce driver so that it notes the output
should be sharded. This is accomplished by adding 'sharded':True to the
output argument:

… out={ 'reduce': ocollection.name, 'sharded': True }) …

Note that the output collection of a mapreduce command, if sharded, must
be sharded using \_id as the shard key.
